---
title: "Practical 8"
author: "Kevin Patyk"
date: "10/26/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
```

# Loading the required libraries

```{r message=FALSE, warning=FALSE}
library(MASS)
library(splines)
library(ISLR)
library(tidyverse)
```

Setting the seed.
```{r}
set.seed(45)
```

# Prediction plot

Median housing prices in Boston do not have a linear relation with the proportion of low SES households. Today we are going to focus exclusively on prediction.
```{r}
Boston %>% 
  ggplot(aes(x = lstat, y = medv)) +
  geom_point() +
  theme_minimal()
```

**1: Create a function called `pred_plot()` that takes as input an `lm` object, which outputs the above plot but with a prediction line generated from the model object using the `predict()` method.**
```{r}
pred_plot <- function(model){
  # First create predictions for all values of lstat
  x_pred <- seq(min(Boston$lstat), max(Boston$lstat), length.out = 500) #new x values 
  y_pred <- predict(model, newdata = tibble(lstat = x_pred)) #now predicting y values
  
  # Then create a ggplot object with a line based on those predictions
  Boston %>%
    ggplot(aes(x = lstat, y = medv)) +
    geom_point() +
    geom_line(data = tibble(lstat = x_pred, medv = y_pred), size = 1, col = "blue") +
    theme_minimal()
}
```

**2: Create a linear regression object called `lin_mod` which models `medv` as a function of `lstat`. Check if your prediction plot works by running `pred_plot(lin_mod)`. Do you see anything out of the ordinary with the predictions?**
```{r}
mod <- lm(medv ~ lstat, data = Boston)
pred_plot(model = mod)
```

The prediction (the line) does not follow the data very well. Additionally, it is below 0 for high values of `lstat`. 

-----

# Polynomial regression

**3: Create another linear model pn3_mod, where you add the second and third-degree polynomial terms `I(lstat^2)` and `I(lstat^3)` to the formula. Create a pred_plot() with this model.**
```{r}
pn3_mod <- lm(medv ~ lstat + I(lstat^2) + I(lstat^3), data = Boston)
pred_plot(model = pn3_mod)
```

The function `poly()` can automatically generate a matrix which contains columns with polynomial basis function outputs.

**4: Play around with the `poly()` function. What output does it generate with the arguments `degree = 3` and `raw = TRUE`?**
```{r}
poly(x = c(1:3), degree = 3, raw = T)
```

The generates a matrix with 3 columns. The first column is to the power of 1, the second column is the squared version, and the third column is the cubed version. 

**5: Use the `poly()` function directly in the model formula to create a 3rd-degree polynomial regression predicting `medv` using `lstat`. Compare the prediction plot to the previous prediction plot you made. What happens if you change the `poly()` function to `raw = FALSE?`**
```{r warning=FALSE}
pn3_mod2 <- lm(medv ~ lstat + poly(x = lstat, degree = 3, raw = T), data = Boston)
pred_plot(model = pn3_mod2)
```

The plot is exactly the same as the previous prediction plot. By default, with `raw = FALSE`, `poly()` computes an orthogonal polynomial. This does not change the fitted values but you can see whether a certain order in the polynomial significantly improves the regression over the lower orders. You can check the `summary(pn3_mod2)`.

-----

# Piecewise regression

Another basis function we can use is a step function. For example, we can split the `lstat` variable into two groups based on its median and take the average of these groups to predict `medv`.

**6: Create a model called `pw2_mod` with one predictor: `I(lstat <= median(lstat))`. Create a `pred_plot` with this model. Use the coefficients in `coef(pw2_mod)` to find out what the predicted value for a low-lstat neighbourhood is.**
```{r}
pw2_mod <- lm(medv ~ I(lstat <= median(lstat)), data = Boston)
pred_plot(pw2_mod)

coef(pw2_mod)
```

The predicted value for low-lstat neighbourhoods is 16.68 + 11.71 = 28.39.

**7: Use the `cut()` function in the formula to generate a piecewise regression model called `pw5_mod` that contains 5 equally spaced sections. Again, plot the result using `pred_plot`.**
```{r}
pw5_mod <- lm(medv ~ cut(x = lstat, breaks = 5), data = Boston)
pred_plot(pw5_mod)
```

Note that the sections generated by `cut()` are equally spaced in terms of `lstat`, but they do not have equal amounts of data. In fact, the last section has only 9 data points to work with:
```{r}
table(cut(Boston$lstat, 5))
```

**8 - Optional: Create a piecewise regression model `pwq_mod` where the sections are not equally spaced, but have equal amounts of training data. Hint: use the `quantile()` function.**
```{r}
brks <- c(-Inf, quantile(Boston$lstat, probs = c(.2, .4, .6, .8)), Inf)
pwq_mod <- lm(medv ~ cut(lstat, brks), data = Boston)
pred_plot(pwq_mod)
```

And, checking how many observations are in each section.
```{r}
table(cut(Boston$lstat, brks))
```

-----

# Piecewise polynomial regression

Combining piecewise regression with polynomial regression, we can write a function that creates a matrix based on a piecewise cubic basis function:
```{r}
piecewise_cubic_basis <- function(vec, knots = 1) {
  # If there is only one section, just return the 3rd order polynomial
  if (knots == 0) return(poly(vec, degree = 3, raw = TRUE))
  # cut the vector
  cut_vec <- cut(vec, breaks = knots + 1)
  # initialise a matrix for the piecewise polynomial
  out <- matrix(nrow = length(vec), ncol = 0)
  # loop over the levels of the cut vector
  for (lvl in levels(cut_vec)) {
    # temporary vector
    tmp <- vec
    # set all values to 0 except the current section
    tmp[cut_vec != lvl] <- 0
    # add the polynomial based on this vector to the matrix
    out <- cbind(out, poly(tmp, degree = 3, raw = TRUE))
  }
  # return the piecewise polynomial matrix
  out
}
```

**9: This function does not have comments. Copy + paste the function and add comments to each line. To figure out what each line does, you can first create “fake” `vec` and `knots` variables, for example `vec <- 1:20` and `knots <- 2` and try out the lines separately.**
```{r}
#making vec and knots for testing 
vec <- 1:20
knots <- 2

cut_vec <- cut(vec, breaks = knots + 1) #this will set the number of intervals where the vector needs to be cut; in this case it will make k + 1 intervals

out <- matrix(nrow = length(vec), ncol = 0) #this makes an empty matrix with the same amount of rows as the length of the vector and 0 columns - this is the storage that will be used for the loop

for (lvl in levels(cut_vec)) { #this will loop for the amount of levels in the cut vector, which will be = to knots + 1; in the case of 2 knots, it will be 3.
  
    # temporary vector
    tmp <- vec #this just puts the original vector (vec) into a temporary variable called tmp
    
    # set all values to 0 except the current section
    tmp[cut_vec != lvl] <- 0 #this will make all the values 0 except for the current section  that we are working on - so, when it is = 1, the second and third section will be 0; basically, this makes it so we are doing the polynomial regression separately for each section, as is intended 
    
    # add the polynomial based on this vector to the matrix
    out <- cbind(out, poly(tmp, degree = 3, raw = TRUE)) #this will add the polynomials for the section that we are working on. So, if we are working on section 1, it will only make it for that section and not sections 2 and 3. 
}
```

This function will take the input vector, cut it into separate segments where the number of segments is equal to the number of knots + 1. Then, it will create a matrix for storage that has 0 columns and the same number of rows as the input vector. It will then run a for loop the same amount of times as there are levels in the cut vector. Each for loop will create a temporary vector which is the same as the input vector, then it will set any values that are not equal to the current level to 0, and it will then compute third order polynomials for that section. 

**10: Create piecewise cubic models with 1, 2, and 3 knots `(pc1_mod - pc3_mod)` using this piecewise cubic basis function. Compare them using the `pred_plot()` function.**
```{r}
pc1_mod <- lm(medv ~ piecewise_cubic_basis(vec = lstat, knots = 1), data = Boston)
pc2_mod <- lm(medv ~ piecewise_cubic_basis(vec = lstat, knots = 2), data = Boston)
pc3_mod <- lm(medv ~ piecewise_cubic_basis(vec = lstat, knots = 3), data = Boston)

pred_plot(pc1_mod)
pred_plot(pc2_mod)
pred_plot(pc3_mod)
```

-----

# Splines

We’re now going to take out the discontinuities from the piecewise cubic models by creating splines. First, we will manually create a cubic spline with 1 knot at the median by constructing a truncated power basis as per ISLR page 273, equation 7.10.

**11: Create a data frame called `boston_tpb` with the columns `medv` and `lstat` from the Boston dataset.**
```{r}
boston_tpb <- Boston %>%
  select(c(medv, lstat))
```

**12: Now use `mutate` to add squared and cubed versions of the `lstat` variable to this dataset.**
```{r}
boston_tpb <- boston_tpb %>% mutate(lstat2 = lstat^2, lstat3 = lstat^3)
```

**13: Use mutate to add a column `lstat_tpb` to this dataset which is 0 below the median and has value `(lstat - median(lstat))^3` above the median. Tip: you may want to use `ifelse()` within your `mutate()` call.**
```{r}
boston_tpb <- boston_tpb %>% 
  mutate(lstat_tpb = ifelse(lstat >  median(lstat), (lstat - median(lstat))^3, 0))
```

Now we have created a complete truncated power basis for a cubic spline fit.

**14: Create a linear model tpb_mod using the `lm()` function. How many predictors are in the model? How many degrees of freedom does this model have?**
```{r}
tpb_mod <- lm(medv ~ ., data = boston_tpb)
summary(tpb_mod)
```

This model has 5 predictors and 5 degrees of freedom.

The `bs()` function from the splines package does all the work for us that we have done in one function call.

**15: Create a cubic spline model `bs1_mod` with a knot at the median using the `bs()` function. Compare its predictions to those of the `tpb_mod` using the `predict()` function on both models.**
```{r}
bs1_mod <- lm(medv ~ bs(x = lstat, knots = median(lstat)), data = Boston)
summary(bs1_mod)
```

Now, to compare the 2 models based off of the predictions.
```{r}
mean(abs(predict(bs1_mod) - predict(tpb_mod)))
```

The absolute difference between the predicted values is negligible. 

**16: Create a prediction plot from the bs1_mod object using the pred_plot() function.**
```{r}
pred_plot(bs1_mod)
```

Note that this line fits very well, but at the right end of the plot, the curve slopes up. Theoretically, this is unexpected – always pay attention to which predictions you are making and whether that behavior is in line with your expectations.

The last extension we will look at is the natural spline. This works in the same way as the cubic spline, with the additional constraint that the function is required to be linear at the boundaries. The `ns()` function from the splines package is for generating the basis representation for a natural spline.

Natural cubic splines are better than cubic splines because they have less degrees of freedom and also because they do not extrapolate at the ends/boundaries, which is usually the case for cubic splines.

**17: Create a natural cubic spline model `(ns3_mod)` with 3 degrees of freedom using the `ns()` function. Plot it, and compare it to the `bs1_mod`.**
```{r}
ns3_mod <- lm(medv ~ ns(lstat, df = 3), data = Boston)
pred_plot(ns3_mod)
```

Still a good fit but with linear ends.

**18: Plot `lin_mod`, `pn3_mod`, `pw5_mod`, `pc3_mod`, `bs1_mod`, and `ns3_mod` and give them nice titles by adding `+ ggtitle("My title")` to the plot. You may use the function `plot_grid()` from the package `cowplot` to put your plots in a grid.**
```{r}
library(cowplot)
plot_grid(
  pred_plot(mod) + ggtitle("Linear regression"),
  pred_plot(pn3_mod) + ggtitle("Polynomial"),
  pred_plot(pw5_mod) + ggtitle("Piecewise constant"),
  pred_plot(pc3_mod) + ggtitle("Piecewise cubic"),
  pred_plot(bs1_mod) + ggtitle("Cubic spline"),
  pred_plot(ns3_mod) + ggtitle("Natural spline")
)
```

-----

# Programming assignment (optional)

**19: Use 12-fold cross validation to determine which of the 6 methods `(lin, pn3, pw5, pc3, bs1, and ns3)` has the lowest out-of-sample MSE.**
```{r warning=FALSE}
# first create an mse function
mse <- function(y_true, y_pred) mean((y_true - y_pred)^2)

# add a 12 split column to the boston dataset so we can cross-validate
boston_cv <- Boston %>% mutate(split = sample(rep(1:12, length.out = nrow(Boston))))

# prepare an output matrix with 12 slots per method for mse values
output_matrix <- matrix(nrow = 12, ncol = 6) #creating an empty matrix with 12 rows and 6 columns
colnames(output_matrix) <- c("lin", "pn3", "pw5", "pc3", "bs1", "ns3") #renaming the columns of the matrices to the model names 

# loop over the splits, run each method, and return the mse values
for (i in 1:12) {
  train <- boston_cv %>% filter(split != i) #the training set will be the splits not equal to i; so, if i = 1, then the training set will be splits 2-12.
  test  <- boston_cv %>% filter(split == i) #the testing set will be the splits equal to i; so if i = 1, then the testing set will be split 1. 
  
  brks <- c(-Inf, 7, 15, 22, Inf) #creating breaks for the piece-wise model
  
  lin_mod <- lm(medv ~ lstat, data = train) #linear model 
  pn3_mod <- lm(medv ~ poly(lstat, 3), data = train) #polynomial model 
  pw5_mod <- lm(medv ~ cut(lstat, brks), data = train) #piecewise model 
  pc3_mod <- lm(medv ~ piecewise_cubic_basis(lstat, 3), data = train) #piecewise cubic 
  bs1_mod <- lm(medv ~ bs(lstat, knots = median(lstat)), data = train) #spline
  ns3_mod <- lm(medv ~ ns(lstat, df = 3), data = train) #natural spline 
  
  #now, we still store all of the mse values in the output matrix/storage that we created earlier. We will fill the matrix row-wise with the mse values. This will happen 12 times to fill all 12 rows with 6 mse values, each corresponding to the appropriate model 
  output_matrix[i, ] <- c( 
    mse(test$medv, predict(lin_mod, newdata = test)), 
    mse(test$medv, predict(pn3_mod, newdata = test)),
    mse(test$medv, predict(pw5_mod, newdata = test)),
    mse(test$medv, predict(pc3_mod, newdata = test)),
    mse(test$medv, predict(bs1_mod, newdata = test)),
    mse(test$medv, predict(ns3_mod, newdata = test))
  )
}

# this is the comparison of the methods
colMeans(output_matrix)
```

Now, we can show it graphically.
```{r}
tibble(names = as_factor(colnames(output_matrix)), 
       mse   = colMeans(output_matrix)) %>% 
  ggplot(aes(x = names, y = mse, fill = names)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  scale_fill_viridis_d(guide = "none") +
  labs(
    x     = "Method", 
    y     = "Mean squared error", 
    title = "Comparing regression method prediction performance"
  )
```

-----

# End of document

-----

```{r}
sessionInfo()
```

