---
title: "Practical 9"
author: "Kevin Patyk"
date: "11/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
```

# Loading the required packages

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(magrittr)
library(psych)
library(caret)
library(gbm)
library(xgboost)
library(data.table)
library(ggforce)
```

-----

# Importing and splitting data

First, we specify a seed and load the training data. We will use this data to make inferences and to train a prediction model.
```{r message=FALSE, warning=FALSE}
set.seed(45)
df <- read_csv("ILPD.csv", col_names = c("Age", "Gender", "Total_Bilirubin", "Direct_Bilirubin", "Alkaline_Phosphotase", "Alamine_Aminotransferase", "Aspartate_Aminotransferase", "Total_Proteins", "Albumin", "Ratio_Albumin_Globulin", "Disease")) %>%
  mutate(Gender = as_factor(Gender),
         Disease = as_factor(ifelse(Disease == 1, "Healthy", "Disease"))) %>%
  na.omit()
```

Making training and testing data.
```{r}
df_split <- df %>%
  mutate(split = sample(c(rep("train", 500), rep("test", 79))))

df <- df_split %>%
  filter(split == "train") %>%
  select(-split)

test <- df_split %>%
  filter(split == "test") %>%
  select(-split)
```

-----

# Implementing classification models

**1: Get an impression of the data by looking at the structure of the data and creating some descriptive statistics.**
```{r}
head(df)
tail(df)
str(df)
```

Getting descriptives per group.
```{r}
df %>%
  select(-c(Gender, Disease)) %>%
  describeBy(df$Disease, fast = TRUE)
```

It becomes directly clear that there are substantial differences between the diseased and non-diseased in the data.

**2: To further explore the data we work with, create some interesting data visualizations that show whether there are interesting patterns in the data.**
```{r warning=FALSE}
df %>%
  select(-Gender) %>% #remove the gender column 
  pivot_longer(where(is.numeric)) %>% #making the data longer where it is numeric
  ggplot(aes(x = value, col = Disease, fill = Disease)) + #setting aesthetics
  geom_boxplot(alpha = 0.8) + #making a box plot
  facet_wrap(~name, scales = "free") + #wrapping 
  scale_color_brewer(palette = "Paired") + #setting colors 
  scale_fill_brewer(palette = "Paired") + #setting the fill 
  theme_minimal() #setting the theme

df %>%
  select(-Gender) %>% #remove the gender column 
  pivot_longer(where(is.numeric)) %>% #making the data longer where it is numeric
  ggplot(aes(x = value, col = Disease, fill = Disease)) + #setting aesthetics
  geom_density(alpha = 0.8) + #making a density plot 
  facet_wrap(~name, scales = "free") + #wrapping 
  scale_color_brewer(palette = "Paired") + #setting colors 
  scale_fill_brewer(palette = "Paired") + #setting the fill 
  theme_minimal() #setting the theme

prop.table(table(df$Gender, df$Disease), margin = 1) %>% #making a table of proportions of males and females and their disease status 
  as.data.frame %>% #coercing this table to be a data frame 
  select(Gender = Var1, Disease = Var2, `Relative Frequency` = Freq) %>% #renaming the columns of the coerced data frame 
  ggplot(aes(y = `Relative Frequency`, x = Gender, col = Disease, fill = Disease)) + #setting the aesthetics 
  geom_histogram(alpha = 0.8, stat = "identity", position = "dodge") + #naking a histogram 
  scale_fill_brewer(palette = "Paired") + #setting collors 
  scale_color_brewer(palette = "Paired") + #setting the fill
  theme_minimal() #setting the theme 
```

There are some differences between distributions for the two `Disease` categories, but the differences do not seem to be dramatic. Additionally, there are relatively more women with the liver disease than men.

**3: Shortly reflect on the difference between bagging, random forests, and boosting.**

* Bagging: Fit a regression tree to N bootstrap samples of the training data take the average of all classification trees to base predictions on. Note: out-of-bag data can serve as internal validation set.

* Random forest: Similarly to bagging, classification trees are trained on a bootstrap sample of the data. However, the decision trees are trained using less than all features in the data. This is usually m = sqrt(p), where p is the total number of features.  

* Boosting: We build a decision tree sequentially. Given the current tree, we fit a (small) tree on the residuals of the current model, rather than on the outcome Y. 

We are going to apply different machine learning models using the `caret` package.

**4: Apply bagging to the training data, to predict the outcome `Disease`, using the `caret` library.**

*Note:* We first specify the internal validation settings, like so:
```{r}
cvcontrol <- trainControl(method = "repeatedcv", 
                          number = 10,
                          allowParallel = TRUE)
```

The `method = "repeatedcv"` is that `repeatedcv` does exactly that: it repeatedly performs X-fold cross-validation on the training data, i.e. if you specify 5 repeats of 10-fold cross-validation, it will perform 10-fold cross-validation on the training data 5 times, using a different set of folds for each cross-validation.

Repetition and folding are orthogonal concepts. Folding means splitting your data into k similar subsets and using all combinations of k-1 as training and using the remaining fold as testing. Repetition is repeating whatever cross validation procedure you used many times (provided the internal random number generator generates different partitions/subsets - only be sure not to use the same seed for each repetition).

These settings can be inserted within the train function from the `caret` package. Make sure to use the `treebag` method, to specify `cvcontrol` as the `trControl` argument and to set `importance = TRUE`.

```{r}
bag_train <- train(Disease ~ .,
                   data = df, 
                   method = 'treebag',
                   trControl = cvcontrol,
                   importance = TRUE)
```

**5: Interpret the variable importance measure using the `varImp` function on the trained model object.**
```{r}
bag_train %>%
  varImp %>%
  plot
```

**6: Create training set predictions based on the bagged model, and use the `confusionMatrix()` function from the `caret` package to assess it’s performance.**
```{r}
pred_bag <- predict(bag_train, type = "raw")
confusionMatrix(pred_bag, reference = df$Disease)
```

We have achieved a perfect training set performance. However, this shows nothing more than that we have been able to train the model. We need to evaluate our model. 

**7: Now ask for the output of the bagged model. Explain why under both the approaches differ.**
```{r}
bag_train
```

I do not really understand the question and get different results since I cannot get access to the data in the same format as whoever wrote this practical. The `confusionMatrix()` function provides us with measures of how well the classification did using predicted and actual values. It does not seem like the `bag_train` object has predicted values to compare to. Thus, it seems that this accuracy is just for the training data.  

* Accuracy: The amount of correct classifications / the total amount of classifications.
* The train accuracy: The accuracy of a model on examples it was constructed on. 
* The test accuracy is the accuracy of a model on examples it hasn't seen.
* Confusion matrix: A tabulation of the predicted class (usually vertically) against the actual class (thus horizontally).

**8: Fit a random forest to the training data to predict the outcome `Disease`, using the `caret` library.**
```{r}
rf_train <- train(Disease ~ .,
                  data = df, 
                  method = 'rf',
                  trControl = cvcontrol,
                  importance = TRUE)
```

**9: Again, interpret the variable importance measure using the `varImp` function on the trained model object. Do you draw the same conclusions as under the bagged model?**
```{r}
rf_train %>%
  varImp %>%
  plot
```

The random forest model indicates that other variables are more important, as compared to the bagged model.

**10: Output the model output from the random forest. Are we doing better than with the bagged model?**
```{r}
rf_train
```

Yes, the most accurate model indicates that we do just slightly better than with the bagged model. However, this might well be due to chance. 

**11: Now, fit a boosting model using the `caret` library to predict disease status.**
```{r}
gbm_train <- train(Disease ~ .,
                   data = df,
                   method = "gbm",
                   verbose = F,
                   trControl = cvcontrol)
```

**12: Again, interpret the variable importance measure. You will have to call for `summary()` on the model object you just created. Compare the output to the previously obtained variable importance measures.**
```{r}
summary(gbm_train)
```

**13: Output the model output from our gradient boosting procedure. Are we doing better than with the bagged and random forest model?**
```{r}
gbm_train
```

Yes, our best model is doing slightly better then the previous two models. However, this might still be random variation.

For now, we will continue with extreme gradient boosting, although we will use a different procedure.

We will use `xgboost` to train a binary classification model, and create some visualizations to obtain additional insight in our model. We will create the visualizations using `SHAP` (SHapley Additive exPlanations) values, which are a measure of importance of the variables in the model. In fact, `SHAP` values indicate the influence of each input variable on the predicted probability for each person. Essentially, these give an indication of the difference between the predicted probability with and without that variable, for each person’s score.

**14: Download the file `shap.R` from this Github repository.**
```{r message=FALSE, warning=FALSE}
library(devtools)
source_url("https://github.com/pablo14/shap-values/blob/master/shap.R?raw=TRUE")
```

**15: Specify your model as follows, and use it to create predictions on the training data.**
```{r message=FALSE, warning=FALSE}
train_x <- model.matrix(Disease ~ ., df)[,-1]
train_y <- as.numeric(df$Disease) - 1
xgboost_train <- xgboost(data = train_x,
                         label = train_y, 
                         max.depth = 10,
                         eta = 1,
                         nthread = 4,
                         nrounds = 4,
                         objective = "binary:logistic",
                         verbose = 2)



pred <- tibble(Disease = predict(xgboost_train, newdata = train_x)) %>%
  mutate(Disease = factor(ifelse(Disease < 0.5, 1, 2),
                          labels = c("Healthy", "Disease")))

table(pred$Disease, df$Disease)
```

**16: First, calculate the `SHAP` rank scores for all variables in the data, and create a variable importance plot using these values. Interpret the plot.**
```{r message=FALSE}
shap_results <- shap.score.rank(xgboost_train,
                                X_train = train_x,
                                shap_approx = F)

var_importance(shap_results)
```

**17: Plot the SHAP values for every individual for every feature and interpret them.**
```{r}
shap_long <- shap.prep(shap = shap_results,
                       X_train = train_x)

plot.shap.summary(shap_long)

xgb.plot.shap(train_x, features = colnames(train_x), model = xgboost_train, n_col = 3)
```

The first plot shows, for example, that those with a high value for `Direct_Bilirubin` have a lower probability of being diseased. Also, those with a higher age have a lower probability of being diseased, while those with a higher `Albumin` have a higher probability of being diseased.

The second set of plots displays the marginal relationships of the SHAP values with the predictors. This conveys the same information, but in greater detail. The interpretability may be a bit tricky for the inexperienced data analyst.

**:18 Verify which of the models you created in this practical performs best on the test data.**
```{r}
bag_test <- predict(bag_train, newdata = test)
rf_test  <- predict(rf_train, newdata = test)
gbm_test <- predict(gbm_train, newdata = test)
xgb_test <- predict(xgboost_train, newdata = model.matrix(Disease ~ ., test)[,-1]) %>%
  factor(x = ifelse(. < 0.5, 1, 2), levels = c(1,2), labels = c("Healthy", "Disease"))

list(bag_test, 
     rf_test, 
     gbm_test, 
     xgb_test) %>%
  map(~ confusionMatrix(.x, test$Disease))
```

It seems that the random forest we built performs the best on the testing data. 

-----

# End of document

-----

```{r}
session_info()
```
   
   